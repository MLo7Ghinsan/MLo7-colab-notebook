{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpStUilxLsDfOpN9vZT+k+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLo7Ghinsan/MLo7-colab-notebook/blob/main/RVC_notebook_mlo7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Retrieval-based-Voice-Conversion (RVC) NOTEBOOK**\n",
        "\n",
        "Notebook made and maintained MLo7. If there's any error or bug on any part then please report to @MLo7#6969 via discord\n",
        "\n",
        "Notebook updated on: 4/25//2023\n",
        "\n",
        "Update log:\n",
        "+ notebook cleanup\n",
        "+ public release (usable)\n",
        "\n",
        "Coming soon:\n",
        "+ inference section"
      ],
      "metadata": {
        "id": "T0BWhiPxUuD6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# + === + Setup section + === +"
      ],
      "metadata": {
        "id": "klL_pWzHiSbF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oy6rqOUinMKv",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title # Setup\n",
        "#@markdown ###[Install any dependencies and necessary models]\n",
        "from IPython.display import clear_output, display\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "!apt-get -y install build-essential python3-dev ffmpeg\n",
        "!pip install --upgrade setuptools wheel\n",
        "!pip install --upgrade pip\n",
        "!apt -y install -qq aria2\n",
        "!git clone https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI\n",
        "%cd /content/Retrieval-based-Voice-Conversion-WebUI\n",
        "!mkdir -p pretrained uvr5_weights\n",
        "!pip install tqdm\n",
        "!pip install -r requirements.txt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained/D32k.pth -d /content/Retrieval-based-Voice-Conversion-WebUI/pretrained -o D32k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained/D40k.pth -d /content/Retrieval-based-Voice-Conversion-WebUI/pretrained -o D40k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained/D48k.pth -d /content/Retrieval-based-Voice-Conversion-WebUI/pretrained -o D48k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained/G32k.pth -d /content/Retrieval-based-Voice-Conversion-WebUI/pretrained -o G32k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained/G40k.pth -d /content/Retrieval-based-Voice-Conversion-WebUI/pretrained -o G40k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained/G48k.pth -d /content/Retrieval-based-Voice-Conversion-WebUI/pretrained -o G48k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained/f0D32k.pth -d /content/Retrieval-based-Voice-Conversion-WebUI/pretrained -o f0D32k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained/f0D40k.pth -d /content/Retrieval-based-Voice-Conversion-WebUI/pretrained -o f0D40k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained/f0D48k.pth -d /content/Retrieval-based-Voice-Conversion-WebUI/pretrained -o f0D48k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained/f0G32k.pth -d /content/Retrieval-based-Voice-Conversion-WebUI/pretrained -o f0G32k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained/f0G40k.pth -d /content/Retrieval-based-Voice-Conversion-WebUI/pretrained -o f0G40k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained/f0G48k.pth -d /content/Retrieval-based-Voice-Conversion-WebUI/pretrained -o f0G48k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/uvr5_weights/HP2-人声vocals+非人声instrumentals.pth -d /content/Retrieval-based-Voice-Conversion-WebUI/uvr5_weights -o HP2-人声vocals+非人声instrumentals.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/uvr5_weights/HP5-主旋律人声vocals+其他instrumentals.pth -d /content/Retrieval-based-Voice-Conversion-WebUI/uvr5_weights -o HP5-主旋律人声vocals+其他instrumentals.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/hubert_base.pt -d /content/Retrieval-based-Voice-Conversion-WebUI -o hubert_base.pt\n",
        "\n",
        "clear_output()\n",
        "\n",
        "print(\"|\")\n",
        "print(\"|\")\n",
        "print(\"|\")\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# + === + Training section + === +"
      ],
      "metadata": {
        "id": "6vAf6lJGiYud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Extract dataset\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "#@markdown ###[Path to the zip file containing your wav data]\n",
        "raw_data_zip_path = \"/content/drive/MyDrive/RVC_data.zip\"  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Path to extract audio files (it doesn't really matter where; keeping it under content to save space\n",
        "\n",
        "ext_dir = \"/content/dataset\"  #@param {type:\"string\"} # Change this to the directory where you want to extract the WAV files, but tbh, I dont think that its necessary to make it a param but oh well\n",
        "\n",
        "with zipfile.ZipFile(raw_data_zip_path, \"r\") as raw_data_zip:\n",
        "  all_files = raw_data_zip.namelist()\n",
        "  wav_files = [f for f in all_files if f.lower().endswith(\".wav\")]\n",
        "  for wav_file in tqdm(wav_files):\n",
        "    raw_data_zip.extract(wav_file, path=ext_dir)\n",
        "  # adding renaming cus people would make the file name a disaster (why do I just thought of this, I should put it in other files too)\n",
        "  wav_files = [f for f in os.listdir(ext_dir) if f.endswith(\".wav\")]\n",
        "  wav_files.sort()\n",
        "  for i, wav_file in enumerate(wav_files):\n",
        "      numbered_name = f\"{i+1:5}.wav\"\n",
        "      old_shit = os.path.join(ext_dir, wav_file)\n",
        "      renamed_wavs = os.path.join(ext_dir, numbered_name)\n",
        "      os.rename(old_shit, renamed_wavs)\n",
        "\n"
      ],
      "metadata": {
        "id": "UWtOtIeDn1Lv",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Preprocess\n",
        "#@markdown ###[skippable if you are resume training from checkpoint]\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "model_name = \"name-of-your-model\"  #@param {type:\"string\"}\n",
        "save_directory = \"path-to-your-save-folder\" #@param {type:\"string\"}\n",
        "logs_dir = save_directory + \"/\" + model_name\n",
        "sample_rate = \"48kHz (48,000)\" #@param [\"32kHz (32,000)\", \"40kHz (40,000)\", \"48kHz (48,000)\"]\n",
        "if sample_rate == \"24kHz (24,000)\":\n",
        "  rate = 32000\n",
        "elif sample_rate == \"44kHz (44,100)\":\n",
        "  rate = 40000\n",
        "else:\n",
        "  rate = 48000\n",
        "pitch_detector = \"harvest\" #@param [\"parselmouth\", \"harvest\", \"dio\"]\n",
        "if pitch_detector == \"parselmouth\":\n",
        "  pd = \"pm\"\n",
        "elif pitch_detector == \"harvest\":\n",
        "  pd = \"harvest\"\n",
        "else:\n",
        "  pd = \"dio\"\n",
        "thread_count = 8 #@param {type:\"slider\", min:0, max:24, step:1}\n",
        "clear_output_messages = True #@param {type:\"boolean\"}\n",
        "if not os.path.exists(logs_dir):\n",
        "    os.makedirs(logs_dir)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "print(\"Running trainset_preprocess_pipeline_print.py\")\n",
        "!python -u trainset_preprocess_pipeline_print.py {ext_dir} {rate} {thread_count} {logs_dir} True\n",
        "print(\".\")\n",
        "print(\".\")\n",
        "print(\".\")\n",
        "print(\"complete\")\n",
        "print(\"|\")\n",
        "print(\"|\")\n",
        "print(\"|\")\n",
        "print(\"Running extract_f0_print.py\")\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "!python -u extract_f0_print.py {logs_dir} {thread_count} {pitch_detector}\n",
        "print(\".\")\n",
        "print(\".\")\n",
        "print(\".\")\n",
        "print(\"complete\")\n",
        "print(\"|\")\n",
        "print(\"|\")\n",
        "print(\"|\")\n",
        "print(\"Running extract_feature_print.py\")\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "!python -u extract_feature_print.py cpu 1 0 0 {logs_dir}\n",
        "print(\".\")\n",
        "print(\".\")\n",
        "print(\".\")\n",
        "print(\"complete\")\n",
        "if clear_output_messages:\n",
        "  clear_output()\n",
        "else:\n",
        "  pass\n",
        "print(\"============================================\")\n",
        "print(\"|\")\n",
        "print(\"|\")\n",
        "print(\"|\")\n",
        "print(\"Preprocessing | finished\")\n"
      ],
      "metadata": {
        "id": "dMwVk4pjvoQI",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Training\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "#@markdown ###[Adjustable parameters]:\n",
        "model_name = \"name-of-your-model\"  #@param {type:\"string\"}\n",
        "save_directory = \"path-to-your-save-folder\" #@param {type:\"string\"}\n",
        "logs_dir = save_directory + \"/\" + model_name\n",
        "sample_rate = \"48kHz (48,000)\" #@param [\"32kHz (32,000)\", \"40kHz (40,000)\", \"48kHz (48,000)\"]\n",
        "if sample_rate == \"24kHz (24,000)\":\n",
        "  rate = \"32k\"\n",
        "elif sample_rate == \"44kHz (44,100)\":\n",
        "  rate = \"40k\"\n",
        "else:\n",
        "  rate = \"48k\"\n",
        "batch_size = 32 #@param {type:\"slider\", min:0, max:150, step:2}\n",
        "finish_epoch = 1200 #@param {type:\"slider\", min:0, max:10000, step:100}\n",
        "save_epoch_interval = 100 #@param {type:\"slider\", min:0, max:1000, step:10}\n",
        "\n",
        "#@markdown ####Notes:\n",
        "#@markdown - 1) Keep save_epoch_interval value lower than finish_epoch if you want to save G and D checkpoints\n",
        "#@markdown - 2) Finish epoch is the MAX epoch that your model will train to for making the final model\n",
        "#@markdown ___\n",
        "\n",
        "#@markdown ###[Mainly keep default]:\n",
        "\n",
        "use_gpu = \"0\"  #@param {type:\"string\"}\n",
        "cache_data = 1  #@param {type:\"integer\"}\n",
        "only_latest = 0  #@param {type:\"integer\"}\n",
        "\n",
        "%cd /content/Retrieval-based-Voice-Conversion-WebUI\n",
        "#%load_ext tensorboard\n",
        "#%tensorboard --logdir {logs_dir}\n",
        "exp_dir = logs_dir\n",
        "os.makedirs(exp_dir, exist_ok=True)\n",
        "\n",
        "old_pt = logs_dir + \"/0_gt_wavs\"\n",
        "!rm -rf {old_pt}/*.pt\n",
        "\n",
        "gt_wavs_dir = f\"{exp_dir}/0_gt_wavs\"\n",
        "co256_dir = f\"{exp_dir}/3_feature256\"\n",
        "f0_dir = f\"{exp_dir}/2a_f0\"\n",
        "f0nsf_dir = f\"{exp_dir}/2b-f0nsf\"\n",
        "dir_list = [gt_wavs_dir, co256_dir, f0_dir, f0nsf_dir]\n",
        "\n",
        "file_names = []\n",
        "for file in os.listdir(gt_wavs_dir):\n",
        "    if os.path.isfile(os.path.join(gt_wavs_dir, file)):\n",
        "        file_name = os.path.splitext(file)[0]\n",
        "        file_names.append(file_name)\n",
        "opt = []\n",
        "\n",
        "for name in file_names:\n",
        "    args = [\n",
        "        f\"{gt_wavs_dir}/{name}.wav\",\n",
        "        f\"{co256_dir}/{name}.npy\",\n",
        "        f\"{f0_dir}/{name}.wav.npy\",\n",
        "        f\"{f0nsf_dir}/{name}.wav.npy\",\n",
        "        \"0\"\n",
        "    ]\n",
        "    opt.append(\"|\".join(args))\n",
        "\n",
        "mute_args = [\n",
        "    f\"/content/Retrieval-based-Voice-Conversion-WebUI/logs/mute/0_gt_wavs/mute{rate}.wav\",\n",
        "    \"/content/Retrieval-based-Voice-Conversion-WebUI/logs/mute/3_feature256/mute.npy\",\n",
        "    \"/content/Retrieval-based-Voice-Conversion-WebUI/logs/mute/2a_f0/mute.wav.npy\",\n",
        "    \"/content/Retrieval-based-Voice-Conversion-WebUI/logs/mute/2b-f0nsf/mute.wav.npy\",\n",
        "    \"0\"\n",
        "]\n",
        "opt.append(\"|\".join(mute_args))\n",
        "\n",
        "with open(f\"{exp_dir}/filelist.txt\", \"w\") as f:\n",
        "    f.write(\"\\n\".join(opt))\n",
        "with open(f\"{exp_dir}/filelist.txt\", \"r\") as file:\n",
        "    repl = file.read()\n",
        "    repl = repl.replace(\".wav.wav\", \".wav\")\n",
        "with open(f\"{exp_dir}/filelist.txt\", \"w\") as file:\n",
        "    file.write(repl)\n",
        "\n",
        "search_string = \"torch.save(opt,\"\n",
        "target_line_number = None\n",
        "with open(\"/content/Retrieval-based-Voice-Conversion-WebUI/train/process_ckpt.py\", \"r\") as f:\n",
        "  lines = f.readlines()\n",
        "for i, line in enumerate(lines):\n",
        "  if search_string in line:\n",
        "    target_line_number = i\n",
        "    break\n",
        "if target_line_number is None:\n",
        "  print(f\"Error: could not find target string '{search_string}' in utils.py\")\n",
        "else:\n",
        "  new_line = f'        torch.save(opt, \"%s/{model_name}.pth\" % name)\\n'\n",
        "  lines[target_line_number] = new_line\n",
        "  with open(\"/content/Retrieval-based-Voice-Conversion-WebUI/train/process_ckpt.py\", \"w\") as f:\n",
        "    f.writelines(lines)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "!python train_nsf_sim_cache_sid_load_pretrain.py -e {logs_dir} -sr {rate} -f0 1 -bs {batch_size} -g {use_gpu} -te {finish_epoch} -se {save_epoch_interval} -pg pretrained/f0G{rate}.pth -pd pretrained/f0D{rate}.pth -l {only_latest} -c {cache_data} 2> /dev/null\n",
        "\n",
        "search_string2 = \"inp_root =\"\n",
        "target_line_number2 = None\n",
        "with open(\"/content/Retrieval-based-Voice-Conversion-WebUI/infer/train-index.py\", \"r\") as f:\n",
        "  lines2 = f.readlines()\n",
        "for i, line in enumerate(lines2):\n",
        "  if search_string2 in line:\n",
        "    target_line_number2 = i\n",
        "    break\n",
        "if target_line_number2 is None:\n",
        "  print(f\"Error: could not find target string '{search_string2}' in utils.py\")\n",
        "else:\n",
        "  new_line2 = f'inp_root = \"{logs_dir}/3_feature256\"\\n'\n",
        "  lines2[target_line_number2] = new_line2\n",
        "  with open(\"/content/Retrieval-based-Voice-Conversion-WebUI/infer/train-index.py\", \"w\") as f:\n",
        "    f.writelines(lines2)\n",
        "!python infer/train-index.py 2> /dev/null\n",
        "!mv /content/Retrieval-based-Voice-Conversion-WebUI/infer/added_IVF512_Flat_mi_baseline_src_feat.index {logs_dir}/{model_name}_added_IVF512.index\n",
        "!mv /content/Retrieval-based-Voice-Conversion-WebUI/infer/big_src_feature_mi.npy {logs_dir}/{model_name}_feature.npy\n",
        "!mv /content/Retrieval-based-Voice-Conversion-WebUI/infer/trained_IVF512_Flat_mi_baseline_src_feat.index {logs_dir}/{model_name}_trained_IVF512.index\n",
        "print(\"|\")\n",
        "print(\"|\")\n",
        "print(\"|\")\n",
        "print(\"Training complete!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "7mOLvkEhy0Jz",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# + === + Inference section + === +"
      ],
      "metadata": {
        "id": "fqyDaOpWip10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # coming soon, meanwhile, just use the web-ui\n",
        "!python infer-web.py --colab --pycmd python3"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tGLW7dqPizzs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
